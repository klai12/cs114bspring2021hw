\documentclass[11pt,letterpaper]{article}

\usepackage{amsmath}
\usepackage{hhline}
\usepackage{multirow}

\begin{document}

\title{CS114B (Spring 2021) Written Assignment 2\\Sequence Labeling}
\author{Due April 6, 2021}
\date{}
\maketitle

\section{Hidden Markov Models}

(You may find the discussion in Chapter A of the Jurafsky and Martin book helpful.)\\

\noindent You are given the following short sentences, tagged with parts of speech:\\

\texttt{Alice/NN admired/VB Dorothy/NN}

\texttt{Dorothy/NN admired/VB every/DT dwarf/NN}

\texttt{Dorothy/NN cheered/VB}

\texttt{every/DT dwarf/NN cheered/VB}

\begin{enumerate}

\item Train a hidden Markov model on the above data. Specifically, compute the initial probability distribution $\mathbf{\pi}$:
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline 
$y_1$ & \texttt{NN} & \texttt{VB} & \texttt{DT} \\ 
\hline 
$P(y_1)$ & • & • & • \\ 
\hline 
\end{tabular}
\end{center}

\vspace{11pt}

The transition matrix $\mathbf{A}$:
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline 
\multicolumn{2}{|c|}{\multirow{2}{*}{$P(y_i|y_{i-1})$}} & \multicolumn{3}{|c|}{$y_i$} \\ 
\cline{3-5}
\multicolumn{2}{|c|}{} & \texttt{NN} & \texttt{VB} & \texttt{DT} \\ 
\hline 
\multirow{3}{*}{$y_{i-1}$} & \texttt{NN} & • & • & • \\ 
\cline{2-5}
& \texttt{VB} & • & • & • \\ 
\cline{2-5}
& \texttt{DT} & • & • & • \\ 
\hline 
\end{tabular} 
\end{center}

\vspace{11pt}\newpage

And the emission matrix $\mathbf{B}$:
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline 
\multicolumn{2}{|c|}{\multirow{2}{*}{$P(x_i|y_i)$}} & \multicolumn{3}{|c|}{$y_i$} \\ 
\cline{3-5}
\multicolumn{2}{|c|}{} & \texttt{NN} & \texttt{VB} & \texttt{DT} \\ 
\hline 
\multirow{7}{*}{$x_i$} & \texttt{Alice} & • & • & • \\ 
\cline{2-5}
& \texttt{admired} & • & • & • \\ 
\cline{2-5}
& \texttt{Dorothy} & • & • & • \\ 
\cline{2-5}
& \texttt{every} & • & • & • \\ 
\cline{2-5}
& \texttt{dwarf} & • & • & • \\ 
\cline{2-5}
& \texttt{cheered} & • & • & • \\ 
\cline{2-5}
& \texttt{<UNK>} & • & • & • \\ 
\hline 
\end{tabular} 
\end{center}

Note that you should account for the unknown word \texttt{<UNK>}, but you don't need to account for the start symbol \texttt{<S>} or the stop symbol \texttt{</S>}. There are ways to train the probabilities of \texttt{<UNK>} from the training set, but for this assignment, you can simply let $\textnormal{count}(\texttt{<UNK>},y)=1$ for all tags $y$ (before smoothing). You should use add-1 smoothing on all three tables.

\item Use the forward algorithm to compute the probability of the following sentence:

\texttt{Alice cheered}

As part of your answer, you should fill in the forward trellis below:
\begin{center}
\begin{tabular}{|c|c|c|}
\hline 
 & \texttt{Alice} & \texttt{cheered} \\ 
\hline 
\texttt{NN} & • & • \\ 
\hline 
\texttt{VB} & • & • \\ 
\hline 
\texttt{DT} & • & • \\ 
\hline 
\end{tabular} 
\end{center}

\item Use the Viterbi algorithm to compute the best tag sequence for the following sentence:

\texttt{Goldilocks cheered}

As part of your answer, you should fill in the Viterbi trellis below. You should also keep track of backpointers, either using arrows or in a separate table.
\begin{center}
\begin{tabular}{|c|c|c|}
\hline 
& \texttt{Goldilocks} & \texttt{cheered} \\ 
\hline 
\texttt{NN} & • & • \\ 
\hline 
\texttt{VB} & • & • \\ 
\hline 
\texttt{DT} & • & • \\ 
\hline 
\end{tabular} 
\end{center}

\end{enumerate}\newpage

\section{Structured Perceptrons}

(You may find the discussion in Chapter 7.5.1 of the Eisenstein book helpful.)\\

\noindent Suppose we are given the following weight matrix $\mathbf{\Theta}$:
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline 
\multirow{2}{*}{$\mathbf{\Theta}$} & \multicolumn{3}{|c|}{$y_i=\ldots$} \\ 
\cline{2-4}
& \texttt{NN} & \texttt{VB} & \texttt{DT} \\ 
\hline 
$y_{i-1}=$ \texttt{<S>} & -0.3 & -0.7 & 0.3 \\ 
\hhline{|=|=|=|=|}
$y_{i-1}=$ \texttt{NN} & -0.7 & 0.3 & -0.3 \\ 
\hline 
$y_{i-1}=$ \texttt{VB} & -0.3 & -0.7 & 0.3 \\ 
\hline 
$y_{i-1}=$ \texttt{DT} & 0.3 & -0.3 & -0.7 \\ 
\hhline{|=|=|=|=|}
$x_i=$ \texttt{Alice} & -0.3 & -0.7 & 0.3 \\ 
\hline 
$x_i=$ \texttt{admired} & 0.3 & -0.3 & -0.7 \\ 
\hline 
$x_i=$ \texttt{Dorothy} & -0.3 & 0.3 & -0.7 \\ 
\hline 
$x_i=$ \texttt{every} & -0.7 & -0.3 & 0.3 \\ 
\hline 
$x_i=$ \texttt{dwarf} & 0.3 & -0.7 & -0.3 \\ 
\hline 
$x_i=$ \texttt{cheered} & -0.7 & 0.3 & -0.3 \\ 
\hline 
\end{tabular} 
\end{center}

\noindent (If you think this matrix looks like $\mathbf{\pi}$, $\mathbf{A}$, and $\mathbf{B}$ stacked on top of each other, you are right! Note that we don't need to account for the unknown word \texttt{<UNK>}, and for simplicity, we will ignore the bias term.)

\begin{enumerate}

\item Suppose we are given the following training sentence:

\texttt{Alice/NN admired/VB Dorothy/NN}

\begin{enumerate}

\item Use the Viterbi algorithm to compute the best tag sequence. As part of your answer, you should fill in the Viterbi trellis below. You should also keep track of backpointers, either using arrows or in a separate table.
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline 
& \texttt{Alice} & \texttt{admired} & \texttt{Dorothy} \\ 
\hline 
\texttt{NN} & • & • & • \\ 
\hline 
\texttt{VB} & • & • & • \\ 
\hline 
\texttt{DT} & • & • & • \\ 
\hline 
\end{tabular} 
\end{center}

\vspace{11pt}

\item Update the weight matrix. Use a constant learning rate $\eta=1$.

\end{enumerate}\newpage

\item Suppose we are given the following testing sentence:

\texttt{Alice cheered}

\noindent Use the Viterbi algorithm to compute the best tag sequence. Again, you should fill in the Viterbi trellis below, and keep track of backpointers.
\begin{center}
\begin{tabular}{|c|c|c|}
\hline 
& \texttt{Alice} & \texttt{cheered} \\ 
\hline 
\texttt{NN} & • & • \\ 
\hline 
\texttt{VB} & • & • \\ 
\hline 
\texttt{DT} & • & • \\ 
\hline 
\end{tabular} 
\end{center}

\end{enumerate}

\section*{Submission Instructions}

Please submit your solutions (in PDF format) to LATTE.

\end{document}
